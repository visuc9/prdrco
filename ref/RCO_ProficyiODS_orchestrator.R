library(lubridate)
library(tidyr)
library(dplyr)
library(httr)
library(jsonlite)

if(!exists("Run_Multi_Constraint_Data_Line_Script")){
  Run_Multi_Constraint_Data_Line_Script <- "no"
}
if(!exists("Multi_Constraint_Average_CO_Downtime_per_Number_of_COed_Machines")){
  Multi_Constraint_Average_CO_Downtime_per_Number_of_COed_Machines <- "no"
}


#change datetime field for Maple settings
temp <- StartTime_Analysis
StartTime_Analysis_MES <- paste0(substr(temp,1,5),"-",substr(temp,6,7),"-",substr(temp,8,9),"T",substr(temp,11,20))
StartTime_Analysis_MES <- gsub("'","" ,StartTime_Analysis_MES)
temp <- EndTime_Analysis
EndTime_Analysis_MES <- paste0(substr(temp,1,5),"-",substr(temp,6,7),"-",substr(temp,8,9),"T",substr(temp,11,20))
EndTime_Analysis_MES <- gsub("'","" ,EndTime_Analysis_MES)


#using the site input data, retrieve list of iODS Lines/Units and filter the correct Line/Unit Ids to be later used in REST calls.
rawdata <- GET(paste0("http://",Server_Address,"/API-iODS/api/Filter/getLines"))
lines_in_Proficy <- fromJSON(content(rawdata, "text"), flatten = TRUE)

print("Proficy Lines extracted")

#If running SUD script, temporarily add the MES Legs to [Line_Input_Data] to be able to retrieve lineID and unitIDs for Legs.
if(exists("SUD_specific_RCO_script")){
  if(SUD_specific_RCO_script=="yes"){
    Line_Input_Data_backup <- Line_Input_Data
    Line_Input_Data <- bind_rows(Line_Input_Data,MES_Leg_Data)
  }
}

#[lines_in_Proficy]: list of lines available in Proficy iODS. [Lines]: line names we are interested to take data for. 
temp <- lines_in_Proficy[lines_in_Proficy$lineDesc %in% Lines,]
temp <- merge(x=temp, y=Line_Input_Data, by.x="lineDesc", by.y="MES_Line_Name")

if(Server_Name=="Taicang"){
  temp <- lines_in_Proficy[lines_in_Proficy$lineDesc %in% c(Lines,"MESPACK5"),]
  temp2 <- Line_Input_Data
  temp2$MES_Line_Name[temp2$MES_Line_Name=="Mespack5"] <- "MESPACK5"
  temp <- merge(x=temp, y=temp2, by.x="lineDesc", by.y="MES_Line_Name")
}

#create a string of lineIDs to take data for.
lineIDs <- as.character(temp$lineId[1])
if(nrow(temp)>1){
  for (i in 2:nrow(temp)){
    lineIDs <- paste0(lineIDs,",",temp$lineId[i])
  }
}

#If running SUD script, after retrieving lineID and unitIDs for Legs, revert back to original [Line_Input_Data].
if(exists("SUD_specific_RCO_script")){
  if(SUD_specific_RCO_script=="yes"){
    Line_Input_Data <- Line_Input_Data_backup
  }
}




#LINE DOWNTIME & MACHINE DOWNTIME extract
rawdata <- GET(paste0("http://",Server_Address,"/API-iODS/api/DowntimeUptime/GetRawData?startTime=",StartTime_Analysis_MES,"&endTime=",EndTime_Analysis_MES,
                      "&lines=",lineIDs,
                      "&columns=StartTime,EndTime,Duration,Uptime,Fault,Reason1,Reason1Category,Reason2,Reason2Category,Reason3,Reason3Category,Reason4,Reason4Category,Location,ProdDesc,ProdCode,ProdFam,ProdGroup,ProcessOrder,TeamDesc,ShiftDesc,LineStatus,Comments,PLDesc,PUDesc,IsContraint,RcdIdx,IsStarved,IsBlocked"))
fulldata <- fromJSON(content(rawdata, "text"), flatten = TRUE)

print("Machine Downtime log extracted")

#convert relevant column names to match with Maple column names
names(fulldata)[which(colnames(fulldata)=="StartTime")] <- "START_TIME"
fulldata$START_TIME <- ymd_hms(as.character(fulldata$START_TIME))
names(fulldata)[which(colnames(fulldata)=="ProdCode")] <- "BRANDCODE"
fulldata$BRANDCODE <- as.character(fulldata$BRANDCODE)
names(fulldata)[which(colnames(fulldata)=="Reason1")] <- "CAUSE_LEVELS_1_NAME"
fulldata$CAUSE_LEVELS_1_NAME <- as.character(fulldata$CAUSE_LEVELS_1_NAME)
names(fulldata)[which(colnames(fulldata)=="Reason2")] <- "CAUSE_LEVELS_2_NAME"
fulldata$CAUSE_LEVELS_2_NAME <- as.character(fulldata$CAUSE_LEVELS_2_NAME)
names(fulldata)[which(colnames(fulldata)=="Reason3")] <- "CAUSE_LEVELS_3_NAME"
fulldata$CAUSE_LEVELS_3_NAME <- as.character(fulldata$CAUSE_LEVELS_3_NAME)
names(fulldata)[which(colnames(fulldata)=="Reason4")] <- "CAUSE_LEVELS_4_NAME"
fulldata$CAUSE_LEVELS_4_NAME <- as.character(fulldata$CAUSE_LEVELS_4_NAME)
names(fulldata)[which(colnames(fulldata)=="PLDesc")] <- "LINE"
fulldata$LINE <- as.character(fulldata$LINE)
names(fulldata)[which(colnames(fulldata)=="Duration")] <- "DOWNTIME"
names(fulldata)[which(colnames(fulldata)=="RcdIdx")] <- "DOWNTIME_PK"
fulldata$DOWNTIME_PK <- as.character(fulldata$DOWNTIME_PK)
names(fulldata)[which(colnames(fulldata)=="PUDesc")] <- "MACHINE"
fulldata$MACHINE <- as.character(fulldata$MACHINE)

names(fulldata)[which(colnames(fulldata)=="Uptime")] <- "UPTIME"
names(fulldata)[which(colnames(fulldata)=="TeamDesc")] <- "TEAM"
names(fulldata)[which(colnames(fulldata)=="ShiftDesc")] <- "SHIFT"
names(fulldata)[which(colnames(fulldata)=="Comments")] <- "OPERATOR_COMMENT"
names(fulldata)[which(colnames(fulldata)=="RcdIdx")] <- "DOWNTIME_PK"
fulldata$EndTime <- fulldata$START_TIME + fulldata$DOWNTIME * 60 #[ONUR}: can be potentially removed and replaced with data-type change.

fulldata$UPTIME[is.na(fulldata$UPTIME)] <- 0

#note that iODS downtime log does not have a column specifying Excluded status, so it is generated by looking at [LineStatus] column.
fulldata$IS_EXCLUDED <- ifelse(grepl("PR In",fulldata$LineStatus),0,1)

#store this raw version of downtime log for further processing for Machine Downtime section if used.
Machine_Downtime_Log_raw <- fulldata

#filter only constraint stops for [LINE_DOWNTIME].
fulldata <- fulldata[fulldata$IsContraint==1,]

#determine if the special logic for Multi-Line Constraint should be run (defined by whether the paramaeter is established AND as "yes"), and if yes, eliminate the multiple entries in downtime log if the line has multi-constraint setup.
if(Run_Multi_Constraint_Data_Line_Script=="yes"){
  if(Multi_Constraint_Average_CO_Downtime_per_Number_of_COed_Machines=="no"){
    fulldata$temp <- paste0(fulldata$START_TIME,fulldata$LINE)
    fulldata <- fulldata[!duplicated(fulldata$temp),]
    fulldata <- fulldata[,-which(colnames(fulldata)=="temp")]
  }
}


#eliminate columns not needed for [LINE_DOWNTIME]
fulldata <- fulldata[,-which(colnames(fulldata)=="IsStarved"),]
fulldata <- fulldata[,-which(colnames(fulldata)=="IsBlocked"),]


#generate version of LINE_DOWNTIME to be used in Current/Next Brandcode definition & Gantt data generation & first stop after CO analysis. (It does not exclude idle events, and adds indications whether Idle or Planned Downtime.)
fulldata$Planned_Stop_Check <- ifelse(grepl("-Planned",fulldata$Reason1Category) | fulldata$Reason1Category=="Planned Downtime"
                                      | (grepl("Planned",fulldata$Reason1Category) & !grepl("Unplanned",fulldata$Reason1Category)),1,0)
if(Server_Name=="Urlati BC" | Server_Name=="Cairo"){
  fulldata$Planned_Stop_Check <- ifelse(grepl("-Planned",fulldata$Reason2Category) | fulldata$Reason2Category=="Planned Downtime",1,0)
}
fulldata$Planned_Stop_Check[is.na(fulldata$Planned_Stop_Check)] <- 0

fulldata <- fulldata[!is.na(fulldata$START_TIME),]
#fulldata <- fulldata[fulldata$IsContraint==TRUE,] #this step is needed as the downtime log from iODS includes both constraint and non-constraint lines. (In Maple, LINE_DOWNTIME table is the subset of MACHINE_DOWNTIME table with constraint data filtered.)
fulldata$Idle_Check <- ifelse(fulldata$IS_EXCLUDED==1,1,0)
LINE_DOWNTIME_full <- fulldata
#LINE_DOWNTIME_for_Uptime_after_CO_Analysis <- LINE_DOWNTIME_full
#LINE_DOWNTIME_for_Gantt_charting <- fulldata

#generate version of LINE_DOWNTIME to be used in filtering CO events, (It already filters out any events excluded from production.)
fulldata <- fulldata[grepl("PR In",fulldata$LineStatus),]
LINE_DOWNTIME <- fulldata


#determine number of constraints by looking at unique Machine count per line (if Multi-Constraint line logic is active)
if(Run_Multi_Constraint_Data_Line_Script=="yes"){
  Number_of_Constraints_data <- LINE_DOWNTIME %>% group_by(LINE,MACHINE) %>% summarize(UPTIME=sum(UPTIME))
  Number_of_Constraints_data <- Number_of_Constraints_data %>% group_by(LINE) %>% summarize(Number_of_Constraints = n())
}



#check whether to extract Machine Downtime data. It is determined based on whether any Machine Downtime-related data modeling is needed. Gantt chart data generation and SUD's Machine Stops After CO generation sub-scripts use Machine Level data.
trigger_for_Machine_Downtime_extraction <- FALSE
if(exists("Run_Machine_Level_analysis")){
  if(Run_Machine_Level_analysis=="yes"){
    trigger_for_Machine_Downtime_extraction <- TRUE
  }
}
if(exists("SUD_Run_Machine_Stops_after_CO_analysis")){
  if(SUD_Run_Machine_Stops_after_CO_analysis=="yes"){
    trigger_for_Machine_Downtime_extraction <- TRUE
  }
}

if(trigger_for_Machine_Downtime_extraction){
  #MACHINE DOWNTIME extract - instead of retaking data, we use the previously taked raw data as part of [LINE_DOWNTIME] table generation.
  fulldata <- Machine_Downtime_Log_raw
  
  fulldata <- fulldata[!is.na(fulldata$START_TIME),]
                       
  fulldata$IS_STOP <- ifelse(fulldata$IsBlocked==TRUE | fulldata$IsStarved==TRUE,0,1)
  fulldata <- fulldata[,-c(which(colnames(fulldata)=="IsBlocked"),
                           which(colnames(fulldata)=="IsStarved"))]
  
  
  #note that iODS downtime log does not have a column specifying Excluded status, so it is generated by looking at [LineStatus] column.
  fulldata$IS_EXCLUDED <- ifelse(grepl("PR In",fulldata$LineStatus),0,1)
  
  #generate version used in SUD's machine stops after CO analysis. Includes both constraint and non-constraint machines, excluding Idle events.
  MACHINE_DOWNTIME_full <- fulldata[fulldata$IS_EXCLUDED==0,]
  
  #generate version used in machine-level Gantt data generation. Includes only non-constraint machines if the line not multi-constraint. Excludes Idle events.
  if(Run_Multi_Constraint_Data_Line_Script=="no"){
    fulldata <- fulldata[fulldata$IsContraint!=TRUE,] #this step is needed as the downtime log from iODS includes both constraint and non-constraint lines. (In Maple, LINE_DOWNTIME table is the subset of MACHINE_DOWNTIME table with constraint data filtered.)
  }
  MACHINE_DOWNTIME <- fulldata
  
}






#PRODUCTION LOG extract
#get day starttime
temp <- ymd_hms(StartTime_Analysis_MES)
temp <- date(temp) - 1
StartTime_Analysis_MES_date <- temp
temp <- date(EndTime_Analysis_MES)
EndTime_Analysis_MES_date <- temp
rawdata <- GET(paste0("http://",Server_Address,"/API-iODS/api/Kpi/GetProductionDaysKpis?startTime=",StartTime_Analysis_MES_date,"&endTime=",EndTime_Analysis_MES_date,
                      "&filterType=lines&ids=",lineIDs,"&kpi=ScheduleTime&prOption=PR%20In:"))
fulldata <- fromJSON(content(rawdata, "text"), flatten = TRUE)
fulldata$StartTime <- ymd_hms(fulldata$StartTime)
seconds_day_start <- hour(fulldata$StartTime[1])*60*60 + minute(fulldata$StartTime[1])*60 + second(fulldata$StartTime[1])

#take production log
temp <- ymd_hms(StartTime_Analysis_MES)
temp <- date(temp) - 1
temp <- paste0(temp,"T06:00:00")
StartTime_Analysis_MES <- temp
temp <- ymd_hms(EndTime_Analysis_MES)
temp <- date(temp) + 1
temp <- paste0(temp,"T06:00:00")
EndTime_Analysis_MES <- temp

rawdata <- GET(paste0("http://",Server_Address,"/API-iODS/api/Production/GetRawData?startTime=",StartTime_Analysis_MES,"&endTime=",EndTime_Analysis_MES,
                      "&lines=",lineIDs))
fulldata <- fromJSON(content(rawdata, "text"), flatten = TRUE)

print("Production log extracted")

names(fulldata)[which(colnames(fulldata)=="StartTime")] <- "START_TIME"
fulldata$START_TIME <- ymd_hms(as.character(fulldata$START_TIME))
names(fulldata)[which(colnames(fulldata)=="EndTime")] <- "END_TIME"
fulldata$END_TIME <- ymd_hms(as.character(fulldata$END_TIME))
names(fulldata)[which(colnames(fulldata)=="ProdCode")] <- "BRANDCODE"
fulldata$BRANDCODE <- as.character(fulldata$BRANDCODE)
names(fulldata)[which(colnames(fulldata)=="PLDesc")] <- "LINE"
fulldata$LINE <- as.character(fulldata$LINE)

PRODUCTION_LOG <- fulldata
PRODUCTION_LOG$BRANDCODE <- as.character(PRODUCTION_LOG$BRANDCODE)
PRODUCTION_LOG$LINE <- as.character(PRODUCTION_LOG$LINE)



#RUNTIME PER DAY data generation
temp <- ymd_hms(StartTime_Analysis_MES)
temp <- date(temp) - 1
StartTime_Analysis_MES_date <- temp
temp <- date(EndTime_Analysis_MES)
EndTime_Analysis_MES_date <- temp

# [NIK]: Obsolite based on comments from Michele Mina - IGNORE
rawdata <- GET(paste0("http://",Server_Address,"/API-iODS/api/Kpi/GetProductionDaysKpis?startTime=",StartTime_Analysis_MES_date,"&endTime=",EndTime_Analysis_MES_date,
                      "&filterType=lines&ids=",lineIDs,"&kpi=ScheduleTime&prOption=PR%20In:"))
fulldata <- fromJSON(content(rawdata, "text"), flatten = TRUE)

print("Scheduled Time data extracted")

names(fulldata)[which(colnames(fulldata)=="StartTime")] <- "START_TIME"
fulldata$START_TIME <- ymd_hms(as.character(fulldata$START_TIME))
fulldata$Date <- date(fulldata$START_TIME)
names(fulldata)[which(colnames(fulldata)=="ScheduleTime")] <- "DURATION"
fulldata$DURATION <- as.numeric(fulldata$DURATION)
fulldata <- merge(x= fulldata, y= lines_in_Proficy, by.x="PLId", by.y="lineId")
names(fulldata)[which(colnames(fulldata)=="lineDesc")] <- "LINE"

Runtime_per_Day_data <- fulldata %>% group_by(Date, LINE) %>% summarize(Runtime = sum(DURATION))



if(!exists("SUD_specific_RCO_script")){
  SUD_specific_RCO_script <- "no"
  
  rawdata <- GET(paste0("http://",Server_Address,"/API-iODS/api/Kpi/GetProductionDaysKpis?startTime=",StartTime_Analysis_MES_date,"&endTime=",EndTime_Analysis_MES_date,
                        "&filterType=lines&ids=",lineIDs,"&kpi=MSU&prOption=PR%20In:"))
  fulldata <- fromJSON(content(rawdata, "text"), flatten = TRUE)
  names(fulldata)[which(colnames(fulldata)=="StartTime")] <- "START_TIME"
  fulldata$START_TIME <- ymd_hms(as.character(fulldata$START_TIME))
  fulldata$Date <- date(fulldata$START_TIME)
  names(fulldata)[which(colnames(fulldata)=="MSU")] <- "Production_MSU"
  fulldata$Production_MSU <- as.numeric(fulldata$Production_MSU)
  fulldata <- merge(x= fulldata, y= lines_in_Proficy, by.x="PLId", by.y="lineId")
  names(fulldata)[which(colnames(fulldata)=="lineDesc")] <- "LINE"
  MSU_per_Day_data <- fulldata %>% group_by(Date, LINE) %>% summarize(Production_MSU = sum(Production_MSU))
  
  Runtime_per_Day_data <- merge(Runtime_per_Day_data,MSU_per_Day_data,by=c("Date","LINE"),all.x=TRUE)
  
  rm(SUD_specific_RCO_script)
}






# [NIK]: Added a version based on GetRawData api iODS call -- excluded by feedback from Alvaro
# rawdata <- GET(paste0("http://",Server_Address,"/API-iODS/api/Production/GetRawData?startTime=",StartTime_Analysis_MES_date,"&endTime=",EndTime_Analysis_MES_date,
#                       "&lines=",lineIDs, "&columns=PLDesc,StartTime,TeamDesc,ShiftDesc,ScheduledTime"))
# 
# fulldata <- fromJSON(content(rawdata, "text"), flatten = TRUE)
# fulldata <- fulldata[, !(names(fulldata) %in% c("FlexibleVariables"))]
# 
# names(fulldata)[which(colnames(fulldata)=="StartTime")] <- "START_TIME"
# fulldata$START_TIME <- ymd_hms(as.character(fulldata$START_TIME))
# fulldata$Date <- date(fulldata$START_TIME)
# names(fulldata)[which(colnames(fulldata)=="ScheduledTime")] <- "DURATION"
# fulldata$DURATION <- as.numeric(fulldata$DURATION)
# 
# names(fulldata)[which(colnames(fulldata)=="PLDesc")] <- "LINE"
# 
# 
# fulldata <- fulldata[!(fulldata$TeamDesc %in% c('NoSchedule', 'NotShift')),]
# fulldata <- fulldata %>% group_by(Date, LINE, ShiftDesc, START_TIME) %>% summarize(DURATION = max(DURATION))
# 
# Runtime_per_Day_data_ <- fulldata %>% group_by(Date, LINE) %>% summarize(Runtime = sum(DURATION))
# fulldata <- fulldata %>% group_by(Date, LINE, ShiftDesc) %>% summarize(START_TIME = min(START_TIME))

fulldata <- fulldata %>% group_by(Date, LINE) %>% summarize(START_TIME = min(START_TIME))


#get Day startTime per Line - this is needed to automatically detect per line, at what hour the production day starts. this info is later used in PowerBI when grouping COs in production days.
fulldata$Shift_Start_hours <- as.numeric(difftime(fulldata$START_TIME,fulldata$Date,units="hours"))
fulldata <- fulldata %>% group_by(LINE,Shift_Start_hours) %>% summarize(tally=n())
temp <- fulldata %>% group_by(LINE) %>% summarize(max_tally=max(tally))
fulldata <- merge(x=fulldata,y=temp,by="LINE")
fulldata <- fulldata[fulldata$tally>fulldata$max_tally/2,]
Day_StartTime_per_Line <- fulldata %>% group_by(LINE) %>% summarize(Day_Start_hours=min(Shift_Start_hours))
if(Server_Name=="Gebze HDW"){
  Day_StartTime_per_Line <- fulldata %>% group_by(LINE) %>% summarize(Day_Start_hours=max(Shift_Start_hours))
}



#BRANDCODE data generation - this part is more complex vs Maple, as iODS does not have a [BRANDCODE] table. Here we are trying to re-create it from the columns available from [PRODUCTION_LOG], as well as re-creating some other columns via site-specific rules.

#Create BRANDCODE meta data from PRODUCTION LOG
temp <- PRODUCTION_LOG[,c(which(colnames(PRODUCTION_LOG)=="BRANDCODE"),
                          which(colnames(PRODUCTION_LOG)=="ProdDesc"),
                          which(colnames(PRODUCTION_LOG)=="ProdFam"),
                          which(colnames(PRODUCTION_LOG)=="ProdGroup"),
                          which(colnames(PRODUCTION_LOG)=="FirstPackCount"),
                          which(colnames(PRODUCTION_LOG)=="StatFactor"))]

temp2 <- temp %>% group_by(BRANDCODE,ProdDesc,ProdFam,ProdGroup,FirstPackCount) %>% summarize(tally=n())
temp2 <- temp2[order(temp2$tally, decreasing = TRUE),]
temp2 <- temp2[!duplicated(temp2$BRANDCODE),]

temp3 <- temp[temp$StatFactor>0,]
temp3 <- temp3 %>% group_by(BRANDCODE) %>% summarize(StatFactor = max(StatFactor))
temp3 <- temp3[!is.na(temp3$BRANDCODE),]
temp2 <- merge(x= temp2, y= temp3, by= "BRANDCODE", all.x= TRUE)
temp2$ProdDesc <- gsub("-",":",temp2$ProdDesc)
temp2 <- temp2 %>% separate(ProdDesc, c("A","B"), sep = ":", remove = FALSE)
temp2$B <- ifelse(is.na(temp2$B),temp2$A,temp2$B)
temp3 <- unique(temp2$B)
if(Server_Name=="Gebze HDW"){
  names(temp2)[which(colnames(temp2)=="ProdDesc")] <- "BRANDNAME"
} else {
  if (length(temp3)>2){
    names(temp2)[which(colnames(temp2)=="B")] <- "BRANDNAME"
  } else {
    names(temp2)[which(colnames(temp2)=="ProdDesc")] <- "BRANDNAME"
  }
}
BRANDCODE_data <- temp2[,c(which(colnames(temp2)=="BRANDCODE"),
                           which(colnames(temp2)=="BRANDNAME"),
                           which(colnames(temp2)=="ProdFam"),
                           which(colnames(temp2)=="ProdGroup"),
                           which(colnames(temp2)=="FirstPackCount"),
                           which(colnames(temp2)=="StatFactor"))]
names(BRANDCODE_data)[which(colnames(BRANDCODE_data)=="FirstPackCount")] <- "UNITS_PER_CASE"
BRANDCODE_data$Server <- Server_Name

#add [SIZE] and make fixes per server if needed
if (Server_Name=="Tabler HDW"){
  temp2 <- BRANDCODE_data %>% separate(BRANDNAME, c("A","B"), sep = "/", remove = FALSE)
  temp2 <- temp2 %>% separate(B, c("C","D"), sep = " ", remove = FALSE)
  temp2$Case_Count <- ""
  for (i in 1:nrow(temp2)){
    temp3 <- gregexpr(pattern =" ",temp2$A[i])
    char_loc <- temp3[[1]][length(temp3[[1]])]
    temp3 <- substr(temp2$A[i],char_loc+1,nchar(temp2$A[i]))
    temp2$Case_Count[i] <- temp3
  }
  temp2$Case_Count <- as.numeric(temp2$Case_Count)
  temp2$UNITS_PER_CASE <- ifelse(is.na(temp2$UNITS_PER_CASE),temp2$Case_Count,temp2$UNITS_PER_CASE)
  names(temp2)[which(colnames(temp2)=="C")] <- "SIZE"
  
  temp2 <- temp2[,c(which(colnames(temp2)=="BRANDCODE"),
                    which(colnames(temp2)=="UNITS_PER_CASE"),
                    which(colnames(temp2)=="SIZE"))]
  BRANDCODE_data <- BRANDCODE_data[,-c(which(colnames(BRANDCODE_data)=="UNITS_PER_CASE"))]
  BRANDCODE_data <- merge(x= BRANDCODE_data, y= temp2, by="BRANDCODE", all.x= TRUE)
} else if (Server_Name=="StLouis"){
  temp <- BRANDCODE_data
  temp$BRANDNAME <- gsub("w/Oxi","wOxi",temp$BRANDNAME)
  temp2 <- temp %>% separate(BRANDNAME, c("A","B"), sep = "/", remove = FALSE)
  temp2 <- temp2 %>% separate(B, c("B","C"), sep = " ", remove = FALSE)
  temp2$B <- gsub("ct","",temp2$B)
  temp2$B <- as.numeric(temp2$B)
  temp2$A <- substr(temp2$A,nchar(temp2$A),nchar(temp2$A))
  temp2$A <- as.numeric(temp2$A)
  names(temp2)[which(colnames(temp2)=="A")] <- "UNITS_PER_CASE"
  names(temp2)[which(colnames(temp2)=="B")] <- "SIZE"
  
  temp2 <- temp2[,c(which(colnames(temp2)=="BRANDCODE"),
                    which(colnames(temp2)=="UNITS_PER_CASE"),
                    which(colnames(temp2)=="SIZE"))]
  BRANDCODE_data <- BRANDCODE_data[,-c(which(colnames(BRANDCODE_data)=="UNITS_PER_CASE"))]
  BRANDCODE_data <- merge(x= BRANDCODE_data, y= temp2, by="BRANDCODE", all.x= TRUE)
  
}  else if (Server_Name=="Gebze HDW"){
  temp <- BRANDCODE_data
  temp2 <- temp %>% separate(BRANDNAME, c("A","B"), sep = "X", remove = FALSE)
  temp2$temp <- substr(temp2$B,1,6)
  temp2$temp2 <- ifelse(grepl("ML",temp2$temp),"ML",ifelse(grepl("L",temp2$temp),"L",""))
  temp2 <- temp2 %>% separate(temp, c("B","C"), sep = "ML", remove = FALSE)
  temp2 <- temp2 %>% separate(B, c("B","C"), sep = "L", remove = FALSE)
  temp2$B <- gsub("[^0-9.-]", "", temp2$B)
  temp2$B <- as.numeric(temp2$B)
  temp2$Size_Raw <- temp2$B
  temp2$B[is.na(temp2$B)] <- 0
  for (i in 1:nrow(temp2)){
    if(temp2$temp2[i]=="L" || temp2$B[i]<100){
      temp2$Size_Raw[i] <- temp2$Size_Raw[i]*1000
    }
    if(is.na(temp2$temp[i])){
      temp3 <- as.numeric(gsub("[^0-9.-]", "", temp2$BRANDNAME[i]))
      if(is.na(temp3)){temp3<-0}
      if(temp3>100){
        temp2$Size_Raw[i] <- temp3
      } else {
        temp2$Size_Raw[i] <- temp3*1000
      }
    }
  }
  names(temp2)[which(colnames(temp2)=="Size_Raw")] <- "SIZE"
  temp2 <- temp2[,c(which(colnames(temp2)=="BRANDCODE"),
                    which(colnames(temp2)=="SIZE"))]
  BRANDCODE_data <- merge(x= BRANDCODE_data, y= temp2, by="BRANDCODE", all.x= TRUE)
} else {
  BRANDCODE_data$SIZE <- NA
}


print("RCO ETL started")


#RUN RCO ETL
script_name <- paste0(Root_folder_Master_Scripts,"/RCO_MES_ETL.R")
source(script_name)


print("RCO ETL completed")


